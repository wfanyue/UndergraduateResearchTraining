# Part IX Mixture of Gaussians and the EM algorithm

## Mixture of Gaussian model

Suppose there's a **latent**(hidden/unobserved) **random variable $z$**, and we wish to model the data by specifying a joint distribution that $x^{(i)},z^{(i)}$ are distributed

$P(x^{(i)},z^{(i)})=P(x^{(i)}|z^{(i)})P(z^{(i)})$, where $z^{(i)}\sim \text{Multinomial}(\phi)$, (where $\phi_j\ge0$, $\sum\limits_{j=1}^k\phi_j=1$, and the parameter $\phi_j$ gives $p(z^{(i)}=j)$, $x^{(i)}|z^{(i)}=j\sim N(\mu_j,\Sigma_j)$).

We let $k$ denote the number of values that the $z^{(i)}$'s can take on. Thus, our model posits(假定) that each $x^{(i)}$ was generated by randomly choosing $z^{(i)}\in\{1,\cdots,k\}$, and then $x^{(i)}$'s was drawned from one of $k$ Gaussians depending on $z^{(i)}$. This is called the mixture of Gaussians model.

Differences with GDA: 

- Set $z$ to be $1$ of $k$ values instead of $1$ of $2$ values(multinomial instead of Bernoulli);
- We are using a different $\Sigma_j$ for each Gaussian.
- Random variable $z^{(i)}$ is latent.

If we know the $z^{(i)}$'s, can use MLE:

$l(\phi,\mu,\Sigma)=\sum\limits_{i=1}^m\log p(x^{(i)};\phi,\mu,\Sigma)=\sum\limits_{i=1}^m\log\sum\limits_{z^{(i)}=1}^kp(x^{(i)}|z^{(i)};\mu,\Sigma)p(z^{(i)};\phi)$.

But it is not possible to find MLE of the parameters in closed form.

$l(\phi,\mu,\Sigma)=\sum\limits_{i=1}^m\log p(x^{(i)}|z^{(i)};\mu,\Sigma)+\log p(z^{(i)};\phi)$.

then

$\phi_j=\dfrac{1}{m}\sum\limits_{i=1}^m1\{z^{(i)}=j \}$

$\mu_j=\dfrac{\sum\limits_{i=1}^m1\{z^{(i)}=j \}x^{(i)}}{\sum\limits_{i=1}^m1\{z^{(i)}=j \}}$

$\Sigma_j=\dfrac{\sum\limits_{i=1}^m1\{z^{(i)}=j \}(x^{(i)}-\mu_i)(x^{(i)}-\mu_i)^T}{\sum\limits_{i=1}^m1\{z^{(i)}=j \}}$

## The EM(Expectation-Maximization) algorithm

E-step: (Guess value of $z^{(i)}$'s)

Set $w^{(i)}_j=P(z^{(i)}=j|x^{(i)};\phi,\mu,\Sigma)=\dfrac{P(x^{(i)}|z^{(i)}=j;\mu,\Sigma)P(z^{(i)}=j;\phi)}{\sum\limits_{l=1}^kP(x^{(i)}|z^{(i)}=l;\mu,\Sigma)P(z^{(i)}=l;\phi)}$;

Here,$P(x^{(i)}|z^{(i)}=j;\mu,\Sigma)$ is given by evaluating the density of a Gaussian with mean $\mu_j$ and covariance $\Sigma_j$ at $x^{(i)}$; $P(z^{(i)}=j;\phi)$ is given by $\phi_j$.

The values $w_j^{(i)}$ calculated represent our "soft" guesses (taking values in $[0,1]$ instead of $\{0,1\}$ ) for the values of $z^{(i)}$.

M-step: 

$\phi_j=\dfrac{1}{m}\sum\limits_{i=1}^mw_j^{(i)}$

$\mu_j=\dfrac{\sum\limits_{i=1}^mw_j^{(i)}x^{(i)}}{\sum\limits_{i=1}^mw_j^{(i)}}$

$\Sigma_j=\dfrac{\sum\limits_{i=1}^mw_j^{(i)}(x^{(i)}-\mu_i)(x^{(i)}-\mu_i)^T}{\sum\limits_{i=1}^mw_j^{(i)}}$

The EM-algorithm is also reminiscent of the K-means clustering algorithm, except that instead of the "hard" cluster assignments $c(i)$, we instead have the "soft" assignments $w^{(i)}_
j$. Similar to K-means, it is also susceptible to local optima, so reinitializing at several different initial parameters may be a good idea.

**Jensen's inequality:**

**Theorem.** Let $f$ be a convex(凹)   /   concave(凸) function, and let $X$ be a random variable. Then, $E[f(X)]\ge(EX)$   /   $E[f(X)]\le(EX)$.

If $f$ is strictly convex, then $E[f(X)]=(EX)$ holds true if and only if $X=EX$ with probability $1$ (i.e., if $X$ is a constant).

<img src="note9_Jensen.png" alt="note9_Jensen" style="zoom:40%;" />

##### General description of the EM algorithm:

<img src="note9_EM.png" alt="note9_EM" style="zoom:40%;" />

Suppose we have an estimation problem in which we have a training set $\{x^{(1)},\cdots,x^{(n)}$ \} consisting of $n$ independent examples. We have a latent variable model $p(x,z;\theta)$ with $z$ being the latent variable. The density of $x$ can be obtained by marginalized over the latent variable $z$: $p(x;\theta)=\sum\limits_zp(x,z;\theta)$.

**Log-likelihood:** $l(\theta)=\sum\limits_{i=1}^n\log p(x^{(i)};\theta)=\sum\limits_{i=1}^n\log\sum\limits_{z^{(i)}}p(x^{(i)},z^{(i)};\theta)$, where $p(x,z;\theta)$ is the joint density.

Maximizing $l(\theta)$ explicitly might be difficult, and our strategy will be to instead repeatedly 

construct a lower-bound on $l$ (E-step), and then optimize that lower-bound (M-step).

Consider optimizing the likelihood $\log p(x)$ for **a single example** $x$. Thus, now we aim to optimize $\log p(x;\theta)$ which can be rewritten as $\log p(x;\theta)=\log\sum\limits_zp(x,z;\theta)$.

Let $Q$ be a **distribution** over the possible values of $z$. That is, $\sum\limits_zQ(z)=1$, $Q(z)\ge0$.

Consider the following:
$$
\begin{align}\log p(x;\theta)&=\log\sum_\limits{z}p(x,z;\theta)\\
&=\log\sum\limits_zQ(z)\dfrac{p(x,z;\theta)}{Q(z)}\\
&\ge\sum\limits_zQ(z)\log\dfrac{p(x,z;\theta)}{Q(z)}
\end{align}
$$
The last step of this derivation used Jensen's inequality. (log function is a concave function)

The term $\log\sum\limits_zQ(z)\left[\dfrac{p(x,z;\theta)}{Q(z)}\right]$ in the summation is just an expectation of the quantity $\left[\dfrac{p(x,z;\theta)}{Q(z)}\right]$ with respect to $z$ drawn according to the distribution given by $Q$. By Jensen's inequality, we have $f\left(E_{z\sim Q}\left[\dfrac{p(x,z;\theta)}{Q(z)}\right] \right)\ge E_{z\sim Q}\left[f\left(\dfrac{p(x,z;\theta)}{Q(z)} \right)\right]$, where the "$z\sim Q$" subscripts above indicate that the expectations are with respect to $z$ drawn to $Q$. 

Now, for **any** distribution $Q$, the formula above gives a lower-bound on $\log p(x;\theta)$. It seems natural to try to make the lower-bound tight at that value of $\theta$. I.e., we will make the inequality above hold with equality at our particular value of $\theta$.

For this to be true, we know it is sufficient that the expectation be taken over a "constant"-valued random variable. I.e., we require that $\dfrac{p(x,z;\theta)}{Q(z)}=c$ for some constant $c$ that does not depend on $z$. This can be easily accomplished by choosing $Q(z)\propto p(x,z;\theta)$. Since $\sum\limits_zQ(z)=1$, this further tells us that
$$
\begin{align}Q(z)&=\dfrac{p(x,z;\theta)}{\sum\limits_zp(x,z;\theta)}\\
&=\dfrac{p(x,z;\theta)}{p(x;\theta)}\\
&=p(z|x;\theta)
\end{align}
$$
For convenience, we define the evidence lower bound (ELBO) by $ELBO(x;Q,\theta)=\sum\limits_zQ(z)\log\dfrac{p(x,z;\theta)}{Q(z)}$.

With this, we can re-write inequality as $\forall Q,\theta, x, \log p(x;\theta)\ge ELBO(x;Q,\theta)$.

Now, for this choice of the $Q_i$'s, repeatedly carry out:

(E-step)

For each $i$, set $Q_i(z^{(i)}):=p(z^{(i)}|x^{(i)};\theta)$;

(M-step)

Set $\theta:=\arg\max\limits_{\theta}\sum\limits_{i=1}^nELBO(x^{(i)};Q_i,\theta)=\arg\max\limits_{\theta}\sum\limits_i\sum\limits_{z^{(i)}}Q_i(z^{(i)})\log\dfrac{p(x^{(i)},x^{(i)};\theta)}{Q_i(z^{(i)})}$.

##### Another Interpretation:

E-step: Maximize $J(\theta,Q)$ w.r.t. $Q$;

M-step: Maximize $J(\theta,Q)$ w.r.t. $\theta$.

